{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yourfutureself/BBB-capstone/blob/main/GPU_BBB_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d93e0b1-e0d8-4f27-a524-4e0c320bd1a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as funct\n",
        "import torch.optim as optim\n",
        "#import pickle\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\n",
        "from skimage import io, transform\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "#from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from PIL.Image import fromarray\n",
        "\n",
        "\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import confusion_matrix , accuracy_score, classification_report\n",
        "\n",
        "#import albumentations as A\n",
        "#from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjPgIi_9tDaF"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rajkumarl/people-clothing-segmentation\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUoryZZPsdZE"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source and destination paths\n",
        "source_path = path # '/kaggle/input/people-clothing-segmentation'\n",
        "destination_path = '/content/people-clothing-segmentation'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "#os.makedirs(destination_path, exist_ok=True) # Removed this line to prevent re-creation\n",
        "\n",
        "# Copy the dataset (instead of moving)\n",
        "for item in os.listdir(source_path):\n",
        "    s = os.path.join(source_path, item)\n",
        "    d = os.path.join(destination_path, item)\n",
        "    if os.path.isdir(s):\n",
        "        # Use distutils.dir_util.copy_tree for merging existing directories\n",
        "        # !pip install distutils\n",
        "        from distutils.dir_util import copy_tree\n",
        "        copy_tree(s, d, update=1)  # update=1 to update existing files\n",
        "                                      # and directories\n",
        "    else:\n",
        "        shutil.copy2(s, d)\n",
        "        print(\"used copy2\")\n",
        "\n",
        "print(f\"Dataset copied from '{source_path}' to '{destination_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOw3yg11TiE3"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4250d6a-b378-45b8-99af-c359599c5bb9"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_built() and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oaj0-JExFDQo"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/people-clothing-segmentation/labels.csv')\n",
        "\n",
        "class_map = {}\n",
        "for index, row in df.iterrows():\n",
        "    grayscale_value = row['Unnamed: 0']  # Assuming the column name is 'grayscale_value'\n",
        "    class_name = row['label_list']  # Assuming the column name is 'class_name'\n",
        "    class_map[grayscale_value] = class_name\n",
        "print(class_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIrIRjbXJz0J"
      },
      "source": [
        "#params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WedaeLCFxiLv"
      },
      "outputs": [],
      "source": [
        "class Params:\n",
        "  seed: int = 161\n",
        "  img_dim_x: int = 550\n",
        "  img_dim_y: int = 825\n",
        "  image_dir = os.path.join(destination_path, 'png_images', 'IMAGES')  # Use updated paths\n",
        "  mask_dir = os.path.join(destination_path, 'png_masks', 'MASKS')\n",
        "  mask_files = os.listdir(mask_dir)\n",
        "  image_files = os.listdir(image_dir)\n",
        "\n",
        "  album_bbox_form: str = \"pascal_voc\"\n",
        "  batch_size = int = 16\n",
        "  custom_model = int = 1\n",
        "  learning_rate = float = 0.005\n",
        "  class_map = class_map\n",
        "  class_num = len(class_map)\n",
        "  num_workers: int = 7\n",
        "  num_epochs: int = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqMCqvaA-McI"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "os.makedirs('/content/people-clothing-segmentation/png_images/VAL_IMAGES/', exist_ok=True)\n",
        "os.makedirs('/content/people-clothing-segmentation/png_masks/VAL_MASKS/', exist_ok=True)\n",
        "os.makedirs('/content/people-clothing-segmentation/png_images/TEST_IMAGES/', exist_ok=True)\n",
        "os.makedirs('/content/people-clothing-segmentation/png_masks/TEST_MASKS/', exist_ok=True)\n",
        "\n",
        "train_image_files, val_image_files, train_mask_files, val_mask_files = train_test_split(\n",
        "    Params.image_files, Params.mask_files, test_size=0.1, random_state=Params.seed  # Use your seed value\n",
        ")\n",
        "\n",
        "# Move validation files to validation folders\n",
        "for file_name in val_image_files:\n",
        "  image_number = file_name.split('_')[1].split('.')[0]  # Get 'xxxx' from 'img_xxxx.png'\n",
        "\n",
        "    # Construct the corresponding mask file name\n",
        "  mask_file_name = f\"seg_{image_number}.png\"\n",
        "\n",
        "  mask_source_path = os.path.join(Params.mask_dir, mask_file_name)\n",
        "  if os.path.exists(mask_source_path):\n",
        "      shutil.move(os.path.join(Params.image_dir, file_name),\n",
        "                  os.path.join('/content/people-clothing-segmentation/png_images/VAL_IMAGES/', file_name))\n",
        "      shutil.move(mask_source_path,\n",
        "                  os.path.join('/content/people-clothing-segmentation/png_masks/VAL_MASKS/', mask_file_name))\n",
        "  else:\n",
        "      print(f\"Warning: Mask file not found: {mask_source_path}\")\n",
        "\n",
        "val_image_files = os.listdir('/content/people-clothing-segmentation/png_images/VAL_IMAGES/')\n",
        "val_mask_files = os.listdir('/content/people-clothing-segmentation/png_masks/VAL_MASKS/')\n",
        "test_image_files = os.listdir('/content/people-clothing-segmentation/png_images/TEST_IMAGES/')\n",
        "test_mask_files = os.listdir('/content/people-clothing-segmentation/png_masks/TEST_MASKS/')\n",
        "\n",
        "val_image_files, test_image_files, val_mask_files, test_mask_files = train_test_split(\n",
        "      val_image_files, val_mask_files, test_size=0.5, random_state=Params.seed\n",
        "  )\n",
        "for file_name in test_image_files:\n",
        "  image_number = file_name.split('_')[1].split('.')[0]  # Get 'xxxx' from 'img_xxxx.png'\n",
        "\n",
        "    # Construct the corresponding mask file name\n",
        "  mask_file_name = f\"seg_{image_number}.png\"\n",
        "\n",
        "  if os.path.exists(mask_source_path):\n",
        "      shutil.move(os.path.join('/content/people-clothing-segmentation/png_images/VAL_IMAGES/', file_name),\n",
        "                  os.path.join('/content/people-clothing-segmentation/png_images/TEST_IMAGES/', file_name))\n",
        "      shutil.move(mask_source_path,\n",
        "                  os.path.join('/content/people-clothing-segmentation/png_masks/MASKS/TEST_MASKS/', mask_file_name))\n",
        "  else:\n",
        "      print(f\"Warning: Mask file not found: {mask_source_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVKg1rVbJ19W"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0iCOxhWoFZS"
      },
      "source": [
        "#transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24LQcyvHaO4l"
      },
      "outputs": [],
      "source": [
        "transform_func = transforms.Compose([\n",
        "    transforms.Resize((Params.img_dim_y, Params.img_dim_x)),\n",
        "    transforms.ToTensor(),\n",
        "    #not sure about this v\n",
        "    #transforms.Lambda(lambda x: (x * 3).repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSO0ntrJAOsy"
      },
      "outputs": [],
      "source": [
        "print(Params.class_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEjM55CRn_tC"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaRuKPEQaIIj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "def get_model_global(num_classes):\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT', progress=True, num_classes=91\n",
        "                                                               ) #makes new model?\n",
        "\n",
        "    if Params.custom_model != -1:\n",
        "      in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "      model.roi_heads.box_predictor = FastRCNNPredictor(in_features, Params.class_num)\n",
        "      #model.roi_heads.score_thresh = 0.3\n",
        "\n",
        "    trainable_layers = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Extract layer name (e.g., 'layer4', 'roi_heads')\n",
        "            layer_name = name.split('.')[0]  # Or adjust based on your model's structure\n",
        "            if layer_name not in trainable_layers:\n",
        "                trainable_layers.append(layer_name)\n",
        "\n",
        "    lr = 5e-3  # Initial learning rate\n",
        "    lr_decay_factor = 0.75\n",
        "    reset_lrBB = 1e-2\n",
        "    reset_lrpred = 5e-3\n",
        "    reset_lrCLS = 5e-3\n",
        "    reset_lrhead = 5e-3\n",
        "    enterBB = 0\n",
        "    enterPred = 0\n",
        "    enterCLS = 0\n",
        "    enterhead = 0\n",
        "    # for idx, (name, param) in enumerate(reversed(list(model.named_parameters()))):\n",
        "    #     # Append layer parameters with decreasing learning rate\n",
        "    #   if any(layer_name in name for layer_name in trainable_layers):\n",
        "\n",
        "    #     if 'backbone' in name and enterBB != 1:\n",
        "    #         lr = reset_lrBB\n",
        "    #         enterBB = 1\n",
        "    #     if 'pred' in name and enterPred != 1:\n",
        "    #         lr = reset_lrpred\n",
        "    #         enterPred = 1\n",
        "    #     if 'cls' in name and enterCLS != 1:\n",
        "    #         lr = reset_lrCLS\n",
        "    #         enterCLS = 1\n",
        "    #     if 'head' in name and enterhead != 1:\n",
        "    #         lr = reset_lrhead\n",
        "    #         enterhead = 1\n",
        "\n",
        "    #     param.requires_grad = True\n",
        "    #     param.lr = lr\n",
        "    #     lr *= lr_decay_factor  # Apply decay for the next layer\n",
        "    #     #print(f'{idx}: lr = {lr:.6f}, {name}')\n",
        "    #   else:\n",
        "    #         param.requires_grad = False  # Freeze other parameters\n",
        "\n",
        "\n",
        "    # for name, param in model.backbone.named_parameters():\n",
        "    #     # Check if the name starts with 'layer' followed by a number\n",
        "    #     if name.startswith('layer') and name[5:].isdigit():\n",
        "    #         layer_num = int(name[5:])  # Extract the layer number\n",
        "    #         if layer_num < 5:  # Freeze layers 0, 1, 2, 3, 4\n",
        "    #             param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "    # Unfreeze later layers of the backbone and the head (RPN, box predictor, mask predictor)\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if 'backbone.layer' not in name or (\n",
        "    #         'backbone.layer' in name and int(name.split('.')[2]) >= 5\n",
        "    #     ):  # Unfreeze layers 3 and above in backbone\n",
        "    #         param.requires_grad = True\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask,\n",
        "        hidden_layer,\n",
        "        Params.class_num\n",
        "    )\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if 'roi_heads' in name or 'mask' in name:\n",
        "    #         param.requires_grad = True\n",
        "            #print(f\"Parameter: {name}, requires_grad: {param.requires_grad}\")\n",
        "    #model.roi_heads.score_thresh = 0.6\n",
        "    #model.roi_heads.mask_thresh = 0.6\n",
        "    model.to(device)\n",
        "    #checkpoint = torch.load('save_model.pth', map_location=device)\n",
        "\n",
        "# Filter the checkpoint to keep only the relevant keys\n",
        "    #filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in model.state_dict()}\n",
        "\n",
        "# Load the filtered state dictionary\n",
        "    #model.load_state_dict(filtered_checkpoint, strict=False)  # Use strict=False to ignore missing keys\n",
        "\n",
        "    # last_25_params = list(model.parameters())[-25:]\n",
        "    # for param in last_25_params:\n",
        "    #   if param.dim() >= 2:\n",
        "    #     torch.nn.init.kaiming_uniform_(param, a=0, mode='fan_in', nonlinearity='relu')\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOY8Tj7hKUFW"
      },
      "outputs": [],
      "source": [
        "def masks_to_boxes_manual(masks):\n",
        "        \"\"\"Converts boolean tensor masks to bounding boxes.\n",
        "\n",
        "        Args:\n",
        "            masks (torch.Tensor): A boolean tensor of shape (N, H, W)\n",
        "                where N is the number of masks, H is the height, and W is the width.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape (N, 4) containing the bounding boxes\n",
        "                in the format (x1, y1, x2, y2) where (x1, y1) is the top-left\n",
        "                corner and (x2, y2) is the bottom-right corner.\n",
        "        \"\"\"\n",
        "\n",
        "        boxes = []\n",
        "        for mask in masks:\n",
        "            # Find the indices where the mask is True\n",
        "            rows, cols = torch.where(mask)\n",
        "\n",
        "            # If the mask is empty, return an empty box\n",
        "            if len(rows) == 0 or len(cols) == 0:\n",
        "                boxes.append(torch.tensor([0, 0, 0, 0], dtype=torch.float32)) # Empty box\n",
        "                continue\n",
        "\n",
        "            # Calculate bounding box coordinates\n",
        "            x1 = cols.min().item()\n",
        "            y1 = rows.min().item()\n",
        "            x2 = cols.max().item()\n",
        "            y2 = rows.max().item()\n",
        "\n",
        "            boxes.append(torch.tensor([x1, y1, x2, y2], dtype=torch.float32))\n",
        "\n",
        "        return torch.stack(boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63TORd5IEG1W"
      },
      "source": [
        "#Dataset maker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqQy7YnOYt9q"
      },
      "outputs": [],
      "source": [
        "#dataset\n",
        "#import scipy\n",
        "from torchvision import transforms, ops\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy import ndimage\n",
        "import re\n",
        "pattern = r\"^img_\\d{4}\\.png$\"\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "class ClothingAndMaskDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=transform_func, class_map=None, batch_size=Params.batch_size):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = os.listdir(image_dir)\n",
        "        self.mask_files = os.listdir(mask_dir)\n",
        "        self.class_map = class_map\n",
        "        self.batch_size = batch_size\n",
        "        self.reverse_class_map = {v: k for k, v in self.class_map.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    import torch\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"in getitme\")\n",
        "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if re.match(pattern, self.image_files[idx]):\n",
        "          image_number = self.image_files[idx].split('_')[1].split('.')[0]\n",
        "          mask_file_name = f\"seg_{image_number}.png\"\n",
        "\n",
        "          mask_path = os.path.join(self.mask_dir, mask_file_name)\n",
        "          if not os.path.exists(mask_path):\n",
        "            raise FileNotFoundError(f\"Could not find mask file: {mask_path}\")\n",
        "\n",
        "          mask = Image.open(mask_path).convert('L')\n",
        "        else:\n",
        "          image_number = self.image_files[idx].split('.')[0]\n",
        "          mask_file_name = f\"{image_number}.png\"\n",
        "          if not os.path.exists(mask_path):\n",
        "            raise FileNotFoundError(f\"Could not find mask file: {mask_path}\")\n",
        "          mask_path = os.path.join(self.mask_dir, mask_file_name)\n",
        "          mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "\n",
        "        #mask = torch.from_numpy(np.array(mask))\n",
        "        # instances are encoded as different colors\n",
        "        #obj_ids = torch.unique(mask)\n",
        "\n",
        "\n",
        "\n",
        "        resize_transform = transforms.Resize((Params.img_dim_y, Params.img_dim_x))\n",
        "        mask = resize_transform(mask)\n",
        "\n",
        "\n",
        "        # image = resize_transform(image)\n",
        "        # print(\"Image shape:\", image.size)\n",
        "        # print(\"Mask shape:\", mask.size)\n",
        "        # plt.figure(figsize=(10, 5))\n",
        "        # plt.subplot(1, 2, 1)\n",
        "        # plt.imshow(image)\n",
        "        # plt.title('Original Image')\n",
        "        # plt.axis('off')\n",
        "        # plt.subplot(1, 2, 2)\n",
        "        # plt.imshow(mask, cmap='gray')  # Display mask in grayscale\n",
        "        # plt.title('Original Mask')\n",
        "        # plt.axis('off')\n",
        "        # plt.show()\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        image = self.transform(image)\n",
        "        labels = []\n",
        "        masks = []\n",
        "        from skimage.morphology import remove_small_objects\n",
        "\n",
        "\n",
        "\n",
        "        # Get unique object IDs from the grayscale mask\n",
        "        for class_id in range(1, Params.class_num):  # Iterate over all class IDs\n",
        "            class_mask = mask == class_id  # Create binary mask for current class\n",
        "\n",
        "            if class_mask.sum() <= 1:  # Check for empty mask\n",
        "              continue\n",
        "            if class_mask.sum() > 1:\n",
        "                # labeled_mask, things_num = ndimage.label(class_mask)\n",
        "                # print(\"things\", things_num)\n",
        "                # for j in range(1, things_num + 1):\n",
        "                #   labeled_mask_j = torch.tensor(labeled_mask == j, dtype=torch.bool)\n",
        "                  labeled_mask_j = torch.tensor(class_mask, dtype=torch.bool)\n",
        "                  if labeled_mask_j.dim() == 1:\n",
        "                    labeled_mask_j = labeled_mask_j.unsqueeze(0)\n",
        "                    #print(\"labeled_mask_j shape:\", labeled_mask_j.shape)\n",
        "                  if labeled_mask_j.dim() == 2:  # Check if it's 2D after unsqueeze\n",
        "                        masks.append(labeled_mask_j)\n",
        "                        labels.append(class_id)\n",
        "                  else:\n",
        "                        print(f\"Skipping mask with unexpected dimensions: {labeled_mask_j.shape}\")\n",
        "        # if idx % 16 == 0:\n",
        "        #   print(len(masks))\n",
        "        if not masks:\n",
        "            print(f\"Warning: No masks found for image {self.image_files[idx]}. Returning empty tensors.\")\n",
        "            # Return empty tensors or handle the case appropriately\n",
        "            # Example: Return image with empty targets\n",
        "            empty_boxes = torch.empty((0, 4), dtype=torch.float32)\n",
        "            empty_labels = torch.empty((0,), dtype=torch.int64)\n",
        "            empty_masks = torch.empty((0, Params.img_dim_y, Params.img_dim_x), dtype=torch.uint8)  # Adjust dimensions if needed\n",
        "            # ... create empty target dictionary ...\n",
        "            target = {\n",
        "                \"boxes\": empty_boxes,\n",
        "                \"labels\": empty_labels,\n",
        "                \"masks\": empty_masks,\n",
        "                \"image_id\": torch.tensor(idx, dtype=torch.int64),\n",
        "                \"area\": torch.zeros((0,), dtype=torch.float32),\n",
        "                \"iscrowd\": torch.zeros((0,), dtype=torch.int64)\n",
        "            }\n",
        "            return image, target\n",
        "\n",
        "\n",
        "\n",
        "        # boxes_list = []  # Create a list to store individual boxes\n",
        "        # for box in boxes:  # Iterate through the detected boxes\n",
        "        #     boxes_list.append([box[0].item(), box[1].item(), box[2].item(), box[3].item()])  # Convert each coordinate to a Python scalar\n",
        "        # boxes = boxes_list\n",
        "        # if not boxes:  # Check if boxes is empty\n",
        "        #   print(f\"Warning: No boxes found for image {self.image_files[idx]}, returning None.\")\n",
        "        #   return None, None\n",
        "\n",
        "\n",
        "\n",
        "        # if idx % 16 == 0:\n",
        "        #    print(\"num boxes after\", len(boxes))\n",
        "        #    print(\"num areas after\", len(area))\n",
        "\n",
        "        filtered_masks = []\n",
        "        filtered_labels = []\n",
        "        #filtered_area = []\n",
        "        #filtered_boxes = []\n",
        "        # {0: nan, 1: 'accessories', 2: 'bag', 3: 'belt', 4: 'blazer', 5: 'blouse', 6: 'bodysuit', 7: 'boots', 8: 'bra', 9: 'bracelet', 10: 'cape', 11: 'cardigan', 12: 'clogs', 13: 'coat', 14: 'dress', 15: 'earrings', 16: 'flats', 17: 'glasses', 18: 'gloves', 19: 'hair', 20: 'hat', 21: 'heels', 22: 'hoodie', 23: 'intimate', 24: 'jacket', 25: 'jeans', 26: 'jumper', 27: 'leggings', 28: 'loafers', 29: 'necklace', 30: 'panties', 31: 'pants', 32: 'pumps', 33: 'purse', 34: 'ring', 35: 'romper', 36: 'sandals', 37: 'scarf', 38: 'shirt', 39: 'shoes', 40: 'shorts', 41: 'skin', 42: 'skirt', 43: 'sneakers', 44: 'socks', 45: 'stockings', 46: 'suit', 47: 'sunglasses', 48: 'sweater', 49: 'sweatshirt', 50: 'swimwear', 51: 't-shirt', 52: 'tie', 53: 'tights', 54: 'top', 55: 'vest', 56: 'wallet', 57: 'watch', 58: 'wedges'}\n",
        "\n",
        "\n",
        "        for mask, label in zip(masks, labels):\n",
        "            class_name = self.class_map.get(label)  # Get class name from label\n",
        "            if class_name not in ['nan', 'watch', 'ring', 'earrings', 'necklace', 'accessories']:  # Check if class name is not \"nan\" or \"skin\"\n",
        "                filtered_masks.append(mask)\n",
        "                filtered_labels.append(label)\n",
        "                #filtered_area.append(area)\n",
        "                #filtered_boxes.append(boxes)\n",
        "\n",
        "        # Update masks and labels with filtered values\n",
        "        masks = filtered_masks\n",
        "        labels = filtered_labels\n",
        "        #area = filtered_area\n",
        "        #boxes = filtered_boxes\n",
        "        if not masks:\n",
        "          print(f\"Warning: No masks found for image {self.image_files[idx]}, returning None.\")\n",
        "          return None, None\n",
        "        # masks = masks[1:]\n",
        "        # labels = labels[1:]\n",
        "        # area = area[1:]\n",
        "        # boxes = boxes[1:]\n",
        "        #add this check:\n",
        "        if len(labels) != len(masks):\n",
        "          print(f\"label filter Warning: Inconsistent lengths for image {self.image_files[idx]}:\"\n",
        "              f\" boxes: {len(boxes)}, labels: {len(labels)}, masks: {len(masks)}\")\n",
        "        image_id = idx\n",
        "\n",
        "        masks = torch.stack(masks)\n",
        "            # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])  # Calculate area\n",
        "        area_threshold = 10\n",
        "        # if idx % 7 == 0:\n",
        "        #   print(\"num boxes\", len(boxes))\n",
        "        #   print(\"num areas\", len(area))\n",
        "        valid_boxes_indices = area > area_threshold\n",
        "        if not valid_boxes_indices.any():\n",
        "            print(f\"Warning: No valid boxes found for image {self.image_files[idx]}, returning None.\")\n",
        "            return None, None\n",
        "        if len(boxes) != len(labels) or len(boxes) != len(masks):\n",
        "          print(f\"box filter Warning: Inconsistent lengths for image {self.image_files[idx]}:\"\n",
        "              f\" boxes: {len(boxes)}, labels: {len(labels)}, masks: {len(masks)}\")\n",
        "\n",
        "        boxes = boxes[valid_boxes_indices]\n",
        "        labels = [label for i, label in enumerate(labels) if valid_boxes_indices[i]]  # Filter labels\n",
        "        masks = masks[valid_boxes_indices]  # Filter masks\n",
        "        area = area[valid_boxes_indices]\n",
        "\n",
        "        # masks = [torch.from_numpy(mask) for mask in masks]\n",
        "        # masks = torch.stack(masks)  # Stack individual masks along a new dimension\n",
        "        # masks = masks.type(torch.bool)  # Convert to Boolean type\n",
        "        #boxes = torch.as_tensor(boxes, dtype=torch.float32)  # Ensure boxes are tensors\n",
        "        labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
        "        iscrowd = torch.zeros(((Params.class_num+1),), dtype=torch.int64).to(device)\n",
        "        image_id = torch.tensor(idx, dtype=torch.int64).to(device)  # or torch.tensor([idx], dtype=torch.int64) for a single-element tensor\n",
        "        area = torch.as_tensor(area, dtype=torch.float32).to(device)\n",
        "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64).to(device)\n",
        "\n",
        "        # boxes_list = []  # Create a list to store individual boxes\n",
        "        # for box in boxes:  # Iterate through the detected boxes\n",
        "        #     boxes_list.append([box[0].item(), box[1].item(), box[2].item(), box[3].item()])\n",
        "        # boxes = boxes_list\n",
        "        #boxes = boxes.to(device).type(torch.float32) # Ensure boxes are float32\n",
        "        boxes.requires_grad = True  # Enable gradient calculation for boxes\n",
        "\n",
        "        #labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
        "        # labels.requires_grad = True # Labels are typically not differentiable\n",
        "\n",
        "        masks = masks.type(torch.float32).to(device) # Ensure masks are float32\n",
        "        masks.requires_grad = True  # Enable gradient calculation for masks\n",
        "\n",
        "        #area = torch.as_tensor(area, dtype=torch.float32).to(device)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(image)).to(device)\n",
        "        target[\"masks\"] = masks.to(device)\n",
        "        target[\"labels\"] = labels.to(device)\n",
        "        #print(labels)\n",
        "        target[\"image_id\"] = image_id.to(device)\n",
        "        target[\"area\"] = area.to(device)\n",
        "        #print(area)\n",
        "        target[\"iscrowd\"] = iscrowd.to(device)\n",
        "        # print(\"Image shape:\", image.shape, \"dtype:\", image.dtype)\n",
        "        # print(\"Bounding boxes:\", target['boxes'])\n",
        "        # print(\"Labels:\", target['labels'])\n",
        "        # print(\"Masks shape:\", target['masks'].shape, \"dtype:\", target['masks'].dtype)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # for targ in target:\n",
        "        #   for k, v in targ.items():\n",
        "        #     if isinstance(v, torch.Tensor) and (torch.isnan(v).any() or torch.isinf(v).any()):\n",
        "        #         print(f\"WARNING: Encountered NaN/Inf in target key '{k}'! Stopping training.\")\n",
        "        #         return None, None\n",
        "        # if idx % 32 == 0:\n",
        "\n",
        "        #     import matplotlib.gridspec as gridspec\n",
        "\n",
        "\n",
        "        #     # Set up the figure with flexible layout\n",
        "        #     num_masks = target['masks'].shape[0]\n",
        "        #     fig = plt.figure(figsize=(4 * num_masks, 6))\n",
        "        #     gs = gridspec.GridSpec(2, num_masks, height_ratios=[1, 2])  # 2 rows: image + masks\n",
        "\n",
        "        #     # --- Top Row: The Processed Image ---\n",
        "        #     ax_image = plt.subplot(gs[0, :])  # Image takes entire top row\n",
        "        #     ax_image.imshow(image.permute(1, 2, 0) * 0.5 + 0.5)\n",
        "        #     ax_image.set_title('Processed Image')\n",
        "        #     ax_image.axis('off')\n",
        "\n",
        "        #     # --- Bottom Row: All Masks ---\n",
        "        #     for i in range(num_masks):\n",
        "        #         ax = plt.subplot(gs[1, i])\n",
        "        #         mask_to_display = target['masks'][i].squeeze().type(torch.float32).cpu().numpy()\n",
        "        #         ax.imshow(mask_to_display, cmap='gray')\n",
        "        #         label_index = target['labels'][i].item()\n",
        "        #         text_label = self.class_map.get(label_index, f\"Unknown Label {label_index}\")\n",
        "        #         ax.set_title(text_label)\n",
        "        #         ax.axis('off')\n",
        "\n",
        "        #     plt.tight_layout()\n",
        "        #     plt.show()\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QybO9XoGqlwM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKApeXE8AGp2"
      },
      "source": [
        "#DataModule (lightning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SFs74zBMiB4f"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning\n",
        "!pip install pytorch-lightning-bolts\n",
        "import pytorch_lightning as pl\n",
        "class Mask_DM(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "        self,\n",
        "        rand_seed= Params.seed,\n",
        "        img_dim_x= Params.img_dim_x,\n",
        "        img_dim_y= Params.img_dim_y,\n",
        "        class_num= Params.class_num,\n",
        "        class_map= Params.class_map,\n",
        "        batch_size= Params.batch_size,\n",
        "        num_workers= Params.num_workers,\n",
        "        transform_func= transform_func,\n",
        "        learning_rate= Params.learning_rate,\n",
        "        num_epochs= Params.num_epochs\n",
        "\n",
        "  ):\n",
        "        super().__init__()\n",
        "        self.rand_seed = rand_seed\n",
        "        self.img_dim_x = img_dim_x\n",
        "        self.img_dim_y = img_dim_y\n",
        "        self.class_num = class_num\n",
        "        self.class_map = class_map\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.transform = transform_func\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "        self.train = ClothingAndMaskDataset(\n",
        "            Params.image_dir,\n",
        "            Params.mask_dir,\n",
        "            transform=self.transform,\n",
        "            class_map=self.class_map,\n",
        "              batch_size=self.batch_size)\n",
        "        self.val = ClothingAndMaskDataset(\n",
        "            os.path.join(destination_path, 'png_images/VAL_IMAGES/'),\n",
        "            os.path.join(destination_path, 'png_masks/VAL_MASKS/'),\n",
        "            transform=self.transform,\n",
        "              class_map=self.class_map,\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "        self.test = ClothingAndMaskDataset(\n",
        "            os.path.join(destination_path, 'png_images/TEST_IMAGES/'),\n",
        "            os.path.join(destination_path, 'png_masks/TEST_MASKS/'),\n",
        "            transform=self.transform,\n",
        "              class_map=self.class_map,\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "  def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.custom_collate\n",
        "        )\n",
        "  def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.custom_collate  # If needed\n",
        "        )\n",
        "  def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            collate_fn=self.custom_collate  # If needed\n",
        "        )\n",
        "\n",
        "\n",
        "  def custom_collate(self, batch):\n",
        "    #print(\"in custom collate\")\n",
        "    imgs = []\n",
        "    targets = []\n",
        "\n",
        "    valid_batch = [(img, targ) for img, targ in batch if img is not None and targ is not None]\n",
        "\n",
        "    if not valid_batch:  # If all samples in the batch are invalid, return empty tensors\n",
        "        print(\"Warning: All samples in the batch are invalid. Returning empty tensors.\")\n",
        "        return None, None  # Return None for both images and targets\n",
        "\n",
        "    for image, target in batch:\n",
        "            if image is None or target is None:\n",
        "                print(\"Warning: Skipping invalid sample in batch.\")  # Add for debugging\n",
        "                continue  # Skip this sample\n",
        "\n",
        "            imgs.append(image)\n",
        "            targets.append(target)\n",
        "    if not imgs or not targets:  # Check if the batch is empty\n",
        "            print(\"Warning: Batch is empty. Returning None.\")\n",
        "            return None, None  # Return None to skip this batch\n",
        "\n",
        "\n",
        "    imgs = torch.stack(imgs)\n",
        "\n",
        "    return imgs, targets  # Return only the valid data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTSg1A1aaAbq"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(self, image, predictions, targets):\n",
        "        # Draw bounding boxes\n",
        "        labels = [str(label.item()) for label in predictions['labels']]\n",
        "        image_with_boxes = draw_bounding_boxes(\n",
        "            image, predictions['boxes'], labels=labels, width=3 # Pass the converted labels\n",
        "        )\n",
        "\n",
        "        # Draw segmentation masks\n",
        "        # ... (get target masks or predictions masks depending on what you want to visualize) ...\n",
        "        target_masks = targets['masks']\n",
        "        image_with_masks = draw_segmentation_masks(image, target_masks, alpha=0.5)\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(image_with_boxes.permute(1, 2, 0))\n",
        "        plt.title('Bounding Boxes')\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(image_with_masks.permute(1, 2, 0))\n",
        "        plt.title('Segmentation Masks')\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gvD_pahxUne"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fts4ZBWIZKnQ"
      },
      "outputs": [],
      "source": [
        "def save_model(model, path):\n",
        "    \"\"\"Saves the model to a file using PyTorch's save function.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to save.\n",
        "        path (str): The path where the model will be saved.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZfkoXmiz89v"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "from torchvision.ops import box_iou\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "class Mask_LM(pl.LightningModule):\n",
        "  def __init__(\n",
        "        self,\n",
        "        rand_seed= Params.seed,\n",
        "        img_dim_x= Params.img_dim_x,\n",
        "        img_dim_y= Params.img_dim_y,\n",
        "        num_classes = Params.class_num,\n",
        "        album_bbox_form= Params.album_bbox_form,\n",
        "        class_map= Params.class_map,\n",
        "        batch_size= Params.batch_size,\n",
        "        num_workers= Params.num_workers,\n",
        "        transform_func= transform_func,\n",
        "        learning_rate= Params.learning_rate,\n",
        "        num_epochs= Params.num_epochs\n",
        "\n",
        "  ):\n",
        "        super().__init__()\n",
        "        self.rand_seed = rand_seed\n",
        "        self.img_dim_x = img_dim_x\n",
        "        self.img_dim_y = img_dim_y\n",
        "        self.num_classes = num_classes\n",
        "        self.album_bbox_form = album_bbox_form\n",
        "        self.class_map = class_map\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.transform = transform_func\n",
        "        self.model = self.get_model(num_classes).to(device)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.optimizers= self.configure_optimizers()\n",
        "        self.optimizer = self.optimizers[\"optimizer\"]\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "\n",
        "  def check_box_predictions(self, images, targets, predictions):\n",
        "      # 1. Check if predictions and targets have boxes and masks\n",
        "      if \"boxes\" not in predictions[0]:\n",
        "            print(\"Warning: Missing 'boxes' in predictions.\")\n",
        "            return False #Changed from True to False\n",
        "      if \"masks\" not in targets[0]:\n",
        "          print(\"Warning: Missing 'masks' in targets.\")\n",
        "          return False #Changed from True to False\n",
        "\n",
        "      # 2. Iterate through the batch\n",
        "      for i in range(len(images)):\n",
        "          target_masks = targets[i][\"masks\"]\n",
        "          predicted_boxes = predictions[i][\"boxes\"]\n",
        "          #print(\"target_masks\", target_masks.shape)\n",
        "          print(\"predicted_boxes\", predictions[0][i]['boxes'].shape[0])\n",
        "\n",
        "          # 3. Check if there are masks but no boxes\n",
        "          if target_masks.shape[0] > 0 and predicted_boxes.shape[0] == 0:\n",
        "              print(f\"Warning: Masks present but no boxes predicted for image {i}.\")\n",
        "              return False  # Or you might want to continue and check other images\n",
        "\n",
        "          # 4. (Optional) Check if box coordinates are within image boundaries\n",
        "          image_height, image_width = images[i].shape[-2:]\n",
        "          for box in predicted_boxes:\n",
        "              x1, y1, x2, y2 = box\n",
        "              if not (0 <= x1 <= image_width and 0 <= y1 <= image_height and\n",
        "                      0 <= x2 <= image_width and 0 <= y2 <= image_height):\n",
        "                  print(f\"Warning: Box coordinates out of bounds for image {i}.\")\n",
        "                  return False  # Or continue to check other boxes\n",
        "\n",
        "      # 5. If all checks pass, consider box predictions reasonable\n",
        "      return True\n",
        "\n",
        "\n",
        "  def calculate_iou_without_background(self, predicted_masks, target_masks, num_classes):\n",
        "    \"\"\"Calculates IoU for each class, excluding the background class (assumed to be class 0).\"\"\"\n",
        "\n",
        "    iou_per_class = []\n",
        "    for class_id in range(1, num_classes):  # Iterate over classes, excluding background (class 0)\n",
        "        # Create binary masks for the current class\n",
        "        predicted_class_mask = predicted_masks == class_id\n",
        "        target_class_mask = target_masks == class_id\n",
        "\n",
        "        # Calculate intersection and union\n",
        "        intersection = (predicted_class_mask & target_class_mask).sum().item()\n",
        "        union = (predicted_class_mask | target_class_mask).sum().item()\n",
        "\n",
        "        # Calculate IoU (handle division by zero)\n",
        "        iou = intersection / union if union else 1.0  # If union is 0, IoU is 1 (perfect match)\n",
        "        iou_per_class.append(iou)\n",
        "\n",
        "    return torch.tensor(iou_per_class)  # Return IoU values for each class\n",
        "  def forward(self, images, targets=None):\n",
        "    # 1. Forward pass\n",
        "    print(\"Entering forward function\")\n",
        "    # If targets are provided, move them to the device before forward pass\n",
        "    if targets is not None:\n",
        "        targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
        "                    for k, v in t.items()}\n",
        "                   for t in targets]\n",
        "\n",
        "    try:\n",
        "        output = self.model(images, targets)\n",
        "        # if self.check_box_predictions(images, targets, output):\n",
        "        #     print(\"Box predictions seem reasonable.\")\n",
        "        #     if not isinstance(output, list):\n",
        "        #         output = [output]  # Wrap in a list\n",
        "        #     print(\"Forward function completed\", output)\n",
        "        #     return output\n",
        "        # else:\n",
        "        #     print(\"Box predictions do not seem reasonable.\")\n",
        "        #     if not isinstance(output, list):\n",
        "        #         output = [output]  # Wrap in a list\n",
        "        #     print(\"Forward function completed\", output)\n",
        "        #     return output\n",
        "    except Exception as e:\n",
        "        print(f\"Exception during forward pass: {e}\")\n",
        "        #print(\"Images shape:\", images.shape, images.dtype, images.device)\n",
        "        if targets is not None:\n",
        "            for i, t in enumerate(targets):\n",
        "                for k, v in t.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        #print(f\"Target {i} - {k}: shape={v.shape}, dtype={v.dtype}, device={v.device}\")\n",
        "                        if torch.isnan(v).any() or torch.isinf(v).any():\n",
        "                            print(f\"WARNING: NaN/Inf found in target {i} - {k}\")\n",
        "        return None  # Or handle the exception in another way\n",
        "\n",
        "    # Wrap in a list if it's not already a list\n",
        "    if not isinstance(output, list):\n",
        "        output = [output]  # Wrap in a list\n",
        "    print(\"Forward function completed\", output)\n",
        "    return output\n",
        "  def get_model(self, num_classes):\n",
        "    #print(\"Getting Model\")\n",
        "    return get_model_global(num_classes)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    #print(\"Training Step\")\n",
        "    images, targets = batch\n",
        "    if images is None or targets is None:\n",
        "        print(\"Skipping batch due to empty tensors.\")  # Add for debugging\n",
        "        return None\n",
        "    images = images.to(device)\n",
        "\n",
        "    # # for overfiting :\n",
        "    # targets_copy = [{k: v.clone().detach().to(device) if isinstance(v, torch.Tensor) else v\n",
        "    #                  for k, v in t.items()}\n",
        "    #                 for t in targets]\n",
        "\n",
        "    # # Now use targets_copy instead of targets\n",
        "    # self.model.train()\n",
        "    # loss_dict = self(images, targets_copy) # Pass the copy to the model\n",
        "\n",
        "\n",
        "    #normal :\n",
        "    for target in targets:  # Loop through each target in the batch\n",
        "        for key, value in target.items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                # if key != 'boxes':\n",
        "                #     # Check for floating-point type and set requires_grad\n",
        "                #     if value.dtype in [torch.float32, torch.float64, torch.float16]:\n",
        "                #         value.requires_grad = True\n",
        "                target[key] = value.to(device)\n",
        "    self.model.train()\n",
        "    #print(\"before forward\")\n",
        "    loss_dict = self(images, targets)\n",
        "    #print(loss_dict)\n",
        "    # Check if loss_dict is None before proceeding\n",
        "    if loss_dict is None:\n",
        "        print(\"Skipping batch due to None loss_dict.\")\n",
        "        return None  # or handle appropriately, e.g., log the event\n",
        "\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    # try ignoring mask loss:\n",
        "    # loss_dict = self(images, targets) # Get the loss dictionary\n",
        "    # del loss_dict[0]['loss_mask']\n",
        "\n",
        "    # for name, param in self.model.named_parameters():\n",
        "    #   print(name, param.requires_grad)\n",
        "\n",
        "    # l1_lambda = 0.001  # Adjust the regularization strength\n",
        "    # l1_reg = torch.tensor(0., device=device)  # Initialize the regularization term\n",
        "    # for name, param in self.model.named_parameters():\n",
        "    #   if 'backbone' in name and 'weight' in name:  # Apply to backbone weights\n",
        "    #     l1_reg += torch.norm(param, 1)  # Calculate L1 norm of weights\n",
        "\n",
        "    #    # Add L1 regularization to the total loss\n",
        "    # total_loss = sum(loss for loss in loss_dict[0].values()) + l1_lambda * l1_reg\n",
        "\n",
        "    weight_classifier = 1.0\n",
        "    weight_box_reg = 2.0  # Increased weight for box regression\n",
        "    weight_mask = 3.0      # Increased weight for mask segmentation\n",
        "    weight_objectness = 0.5\n",
        "    weight_rpn_box_reg = 1.0\n",
        "\n",
        "    loss_classifier = loss_dict[0]['loss_classifier']\n",
        "    loss_box_reg = loss_dict[0]['loss_box_reg']\n",
        "    loss_mask = loss_dict[0]['loss_mask']\n",
        "    loss_objectness = loss_dict[0]['loss_objectness']\n",
        "    loss_rpn_box_reg = loss_dict[0]['loss_rpn_box_reg']\n",
        "\n",
        "    total_loss = (\n",
        "      weight_classifier * loss_classifier +\n",
        "      weight_box_reg * loss_box_reg +\n",
        "      weight_mask * loss_mask +\n",
        "      weight_objectness * loss_objectness +\n",
        "      weight_rpn_box_reg * loss_rpn_box_reg\n",
        "    )\n",
        "    self.log('train_loss', total_loss)  # Log the total loss\n",
        "    # total_loss = sum(loss for loss in loss_dict[0].values()) # Get all loss values from the dictionary and sum them\n",
        "    if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
        "      print(\"WARNING: Encountered NaN/Inf in loss! Stopping training.\")\n",
        "# You might want to log this event or save the model state for debugging\n",
        "      self.trainer.should_stop = True  # Stop training\n",
        "      return None\n",
        "\n",
        "    # predictions = {\n",
        "    #     \"masks\": loss_dict[0]['loss_mask']  # Assuming the predictions are stored under 'masks' key\n",
        "    # }\n",
        "    # iou_without_bg = self.calculate_iou_without_background(\n",
        "    #     predictions[0]['masks'].argmax(dim=0),  # Get predicted masks (assuming multi-class output)\n",
        "    #     targets[0]['masks'].squeeze(1),  # Get target masks\n",
        "    #     self.num_classes\n",
        "    # )\n",
        "    # reward_weight = 3.0  # Adjust this weight to control the reward's influence\n",
        "    # reward_term = torch.exp(reward_weight * iou_without_bg).mean()  # Exponential reward based on IoU\n",
        "\n",
        "    # # 3. Adjust the total loss\n",
        "    # total_loss = total_loss - reward_term  # Subtract the reward from the loss (to maximize reward)\n",
        "\n",
        "    #self.log('train_loss', total_loss)  # Log the total loss\n",
        "    # total_loss = total_loss.item() if isinstance(total_loss, torch.Tensor) else total_loss\n",
        "    # total_loss = torch.tensor(total_loss, device=device).to(device)\n",
        "    #print(\"total loss\", total_loss)\n",
        "\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    # last_20_params = list(reversed(list(self.model.named_parameters())))[-20:]\n",
        "\n",
        "    # print(\"Gradients of the last 20 parameters:\")\n",
        "    # for name, param in last_20_params:\n",
        "    #     if param.grad is not None:  # Check if gradient exists\n",
        "    #         print(f\"{name}: {param.grad}\")  # Print gradient\n",
        "    #         if param.requires_grad == True: print(\"requires grad\")\n",
        "    #         #print(f\"{name}: {param.requires_grad == True}\")\n",
        "    #     else:\n",
        "    #         print(f\"{name}: No gradient\")\n",
        "    # roi_pred_mask_params = [\n",
        "    #     param for name, param in self.model.named_parameters()\n",
        "    #     if 'roi_heads.box_predictor' in name or 'roi_heads.mask_predictor' in name\n",
        "    # ]\n",
        "    # last_20_params = roi_pred_mask_params[-20:]\n",
        "\n",
        "    # print(\"Gradients of the last 20 parameters (including ROI prediction and mask layers):\")\n",
        "    # for i, param in enumerate(last_20_params):  # Iterate through parameters with index\n",
        "    #     if param.grad is not None:  # Check if gradient exists\n",
        "    #         print(f\"Parameter {i}: {param.grad}\")  # Print gradient\n",
        "    #         #print(\"requires grad:\")\n",
        "    #         #print(f\"{name}: {param.requires_grad == True}\")\n",
        "    #     else:\n",
        "    #         print(f\"Parameter {i}: No gradient\")\n",
        "    if (batch_idx % 5) == 0:\n",
        "      print(f\"Batch {batch_idx} Loss: {total_loss.item()}\")\n",
        "      #print(\"loss dict:\", loss_dict)\n",
        "      #print(\"weights\", l1_reg)\n",
        "\n",
        "      # with torch.no_grad():\n",
        "      #       self.model.eval()\n",
        "      #       predictions = self.model(images)\n",
        "      #       accuracy = self.calculate_accuracy(predictions, targets)\n",
        "      #       precision = self.calculate_precision(predictions, targets)\n",
        "\n",
        "      #   # Log the metrics\n",
        "      #       self.log('val_accuracy', accuracy)\n",
        "      #       self.log('val_precision', precision)\n",
        "      #       # Loop through the predictions and targets to visualize them\n",
        "\n",
        "      #       #visualize_predictions(images[1], predictions[1], targets[1])\n",
        "      #       self.model.train()\n",
        "\n",
        "\n",
        "    for param in self.model.parameters():\n",
        "      if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
        "          print(\"WARNING: Encountered NaN/Inf in gradients! Stopping training.\")\n",
        "          self.trainer.should_stop = True\n",
        "          return None\n",
        "    # initial_params = []\n",
        "    # for name, param in reversed(list(self.model.named_parameters())):\n",
        "    #     if len(initial_params) < 20:  # Limit to the first 20 parameters\n",
        "    #         initial_params.append((name, param.clone().detach()))\n",
        "\n",
        "    #adjust lr:\n",
        "    # grad_norm = 0\n",
        "    # for param in self.model.parameters():\n",
        "    #     if param.grad is not None:\n",
        "    #         grad_norm += param.grad.data.norm(2).item()  # Calculate L2 norm\n",
        "    # grad_norm /= len(list(filter(lambda p: p.grad is not None, self.model.parameters())))\n",
        "\n",
        "    # # Define a threshold for gradient norm\n",
        "    # grad_norm_threshold = 1.0  # Adjust this value as needed\n",
        "\n",
        "    # # Reduce learning rate if gradient norm is below the threshold\n",
        "    # if grad_norm < grad_norm_threshold:\n",
        "    #     for param_group in self.optimizer.param_groups:\n",
        "    #         param_group['lr'] *= 0.9\n",
        "\n",
        "\n",
        "\n",
        "    self.optimizer.step()  # Perform the optimizer step\n",
        "\n",
        "    # After optimizer step: Calculate and print the change in weights and biases\n",
        "    # print(\"Change in weights and biases:\")\n",
        "    # for i, (name, initial_param) in enumerate(initial_params):\n",
        "    #     current_param = dict(self.model.named_parameters())[name]\n",
        "    #     change = current_param - initial_param\n",
        "    #     print(f\"{i}. {name}: {change.abs().mean().item()}\")  # Print the average absolute change # Print the average change\n",
        "\n",
        "    save_model(m_lm.model, '/content/save_model.pth')\n",
        "    return total_loss\n",
        "  def test_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        images = images.to(device)\n",
        "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        # Handle cases where the batch might be None (due to filtering in the dataset)\n",
        "        if images is None or targets is None:\n",
        "          print(\"Skipping batch in test_step due to invalid data.\")\n",
        "          return {}\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            predictions = self.model(images)\n",
        "\n",
        "            # 4. Calculate evaluation metrics (if needed)\n",
        "            accuracy = self.calculate_accuracy(predictions, targets)\n",
        "            print(f\"Batch Accuracy (test_step): {accuracy:.4f}\")  # Debugging print\n",
        "            return {\"test_accuracy\": accuracy}\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "        save_model(m_lm.model, '/content/save_model.pth')\n",
        "        images, targets = batch\n",
        "        images = images.to(device)\n",
        "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        if images is None or targets is None:\n",
        "            print(\"Skipping batch in validation_step due to invalid data.\")  # Optional: Add for debugging\n",
        "            return {}\n",
        "        # 1. Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "        score_threshold = 0.1\n",
        "        # 2. Forward pass\n",
        "        assert not self.model.training, \"Model is not in evaluation mode!\"\n",
        "        plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "        def show(imgs):\n",
        "            if not isinstance(imgs, list):\n",
        "                imgs = [imgs]\n",
        "            fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "            for i, img in enumerate(imgs):\n",
        "                img = img.detach()\n",
        "                img = F.to_pil_image(img)\n",
        "                axs[0, i].imshow(np.asarray(img))\n",
        "                axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "        with torch.no_grad():\n",
        "          predictions = self.model(images)\n",
        "            #print(\"Predictions:\", predictions)\n",
        "\n",
        "          #accuracy = self.calculate_accuracy(predictions, targets)\n",
        "\n",
        "            # 5. Log the metric\n",
        "          #self.log('val_accuracy', accuracy)\n",
        "          #image = images[1]\n",
        "          #prediction = predictions[1]\n",
        "          #filtered_boxes = prediction['boxes'][prediction['scores'] > score_threshold]\n",
        "          # Now use the filtered boxes to create the overlay\n",
        "          #overlay = draw_bounding_boxes(image, filtered_boxes, width=5)\n",
        "          # Define the show function within the same cell:\n",
        "\n",
        "          #show(overlay)\n",
        "        return predictions\n",
        "  # Add this to verify your data before training\n",
        "  # def on_train_start(self):\n",
        "  #     # Check a sample batch\n",
        "  #     sample = next(iter(self.train_dataloader()))\n",
        "  #     images, targets = sample\n",
        "  #     print(f\"Batch shape: {images.shape}\")\n",
        "  #     print(f\"Number of targets: {len(targets)}\")\n",
        "  #     print(f\"Sample target keys: {targets[0].keys()}\")\n",
        "\n",
        "  #     # Verify model structure\n",
        "  #     print(\"Model structure:\")\n",
        "  #     print(self.model)\n",
        "\n",
        "  #     # Check which layers are trainable\n",
        "  #     print(\"\\nTrainable parameters:\")\n",
        "  #     for name, param in self.model.named_parameters():\n",
        "  #         if param.requires_grad:\n",
        "  #             print(name)\n",
        "  def configure_optimizers(self):\n",
        "    # 1. Define the optimizer\n",
        "    params = [p for p in self.model.parameters() if p.requires_grad]\n",
        "    #print(\"params from optimizer\", params)\n",
        "    optimizer = torch.optim.SGD(params, lr=self.learning_rate, momentum=0.9)\n",
        "\n",
        "    # 2. Define the scheduler\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "    # 3. Return optimizer and scheduler\n",
        "    return {\n",
        "        \"optimizer\": optimizer,\n",
        "        \"lr_scheduler\": {\n",
        "            \"scheduler\": scheduler,\n",
        "            \"interval\": \"epoch\",  # Adjust frequency: \"epoch\" or \"step\"\n",
        "            \"frequency\": 1  # How often to apply the scheduler\n",
        "        }\n",
        "    }\n",
        "  def calculate_accuracy(self, predictions, targets):\n",
        "    \"\"\"Calculates accuracy based on IoU threshold.\"\"\"\n",
        "    iou_threshold = 0.5  # Set your desired IoU threshold\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for prediction, target in zip(predictions, targets):\n",
        "        predicted_boxes = prediction['boxes']\n",
        "        predicted_labels = prediction['labels']\n",
        "        target_boxes = target['boxes']\n",
        "        target_labels = target['labels']\n",
        "\n",
        "        # Check if predicted_boxes is empty\n",
        "        if predicted_boxes.shape[0] == 0:\n",
        "            # If empty, skip this prediction and continue to the next one\n",
        "            # You might want to log this event or handle it differently\n",
        "            print(\"Warning: No predicted boxes found for this image. Skipping accuracy calculation.\")\n",
        "            continue\n",
        "\n",
        "        # Calculate IoU between predicted and target boxes\n",
        "        iou_matrix = torchvision.ops.box_iou(predicted_boxes, target_boxes)\n",
        "\n",
        "        # Find the best matching target box for each predicted box\n",
        "        matched_indices = iou_matrix.argmax(dim=1)\n",
        "\n",
        "        # Count correct predictions based on IoU and label match\n",
        "        for i, predicted_label in enumerate(predicted_labels):\n",
        "            if iou_matrix[i, matched_indices[i]] >= iou_threshold and predicted_label == target_labels[matched_indices[i]]:\n",
        "                correct_predictions += 1\n",
        "\n",
        "        total_predictions += len(predicted_boxes)\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions else 0\n",
        "    return accuracy\n",
        "\n",
        "  def calculate_precision(self, predictions, targets):\n",
        "    \"\"\"Calculates precision based on IoU threshold.\"\"\"\n",
        "    iou_threshold = 0.5  # Set your desired IoU threshold\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "\n",
        "    for prediction, target in zip(predictions, targets):\n",
        "        predicted_boxes = prediction['boxes']\n",
        "        predicted_labels = prediction['labels']\n",
        "        target_boxes = target['boxes']\n",
        "        target_labels = target['labels']\n",
        "\n",
        "        # Calculate IoU between predicted and target boxes\n",
        "        iou_matrix = torchvision.ops.box_iou(predicted_boxes, target_boxes)\n",
        "\n",
        "        # Find the best matching target box for each predicted box\n",
        "        matched_indices = iou_matrix.argmax(dim=1)\n",
        "\n",
        "        # Count true positives and false positives based on IoU and label match\n",
        "        for i, predicted_label in enumerate(predicted_labels):\n",
        "            if iou_matrix[i, matched_indices[i]] >= iou_threshold and predicted_label == target_labels[matched_indices[i]]:\n",
        "                true_positives += 1\n",
        "            else:\n",
        "                false_positives += 1\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) else 0\n",
        "    return precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSQHvYp9mltV"
      },
      "outputs": [],
      "source": [
        "\n",
        "del m_dm, m_lm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyZL44vvMw12",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "m_dm = Mask_DM()\n",
        "m_dm.learning_rate = Params.learning_rate\n",
        "m_dm.class_map = Params.class_map\n",
        "m_dm.class_num = Params.class_num\n",
        "m_dm.setup()\n",
        "m_lm = Mask_LM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnRkhqLF0ztG"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "#torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLm4Pc1xfqME"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'  # Disable frozen module validation\n",
        "os.environ['PYDEVD_DISABLE_ROOT_VALIDATION'] = '1'  # Disable root module validation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    accelerator = 'cuda'\n",
        "    strategy = 'ddp_notebook' # or 'ddp_spawn' depending on your needs\n",
        "elif torch.backends.mps.is_available():\n",
        "    accelerator = 'mps'\n",
        "    strategy = 'ddp_spawn' # or 'ddp_spawn' depending on your needs\n",
        "else:\n",
        "    accelerator = 'cpu'\n",
        "    strategy = 'auto'"
      ],
      "metadata": {
        "id": "oAqEdxXoI4vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accelerator:\", accelerator)\n",
        "print(\"Strategy:\", strategy)"
      ],
      "metadata": {
        "id": "fSCG1JiiePcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTHx9gHerghk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        " # No strategy needed for CPU training\n",
        "# accelerator = 'cuda'\n",
        "# strategy = 'ddp_notebook'\n",
        "trainer = pl.Trainer(accelerator=accelerator, max_epochs=Params.num_epochs,\n",
        "                    num_sanity_val_steps=0, strategy=strategy)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  trainer.fit(m_lm, train_dataloaders=m_dm.train_dataloader())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtyNR52Gl3Q_"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import draw_bounding_boxes\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "model = get_model_global(num_classes=Params.class_num)  # Create an instance of your model\n",
        "checkpoint = torch.load('save_model.pth', map_location=device)  # Load the checkpoint\n",
        "model.load_state_dict(checkpoint)  # Load the model's state dictionary\n",
        "model.to(device) # Move to appropriate device\n",
        "model.eval()\n",
        "if not os.path.exists('/content/img_0985.png'): uploaded = files.upload()\n",
        "score_threshold = 0.5\n",
        "image = Image.open('/content/img_0985.png')\n",
        "image = transform_func(image)\n",
        "model.eval()\n",
        "output = model([image.to(device)])\n",
        "overlay = draw_bounding_boxes(image, output[0]['boxes'][output[0]['scores'] > score_threshold], width=5)\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "show(overlay)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training, verify predictions on a batch\n",
        "model = m_lm.model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    image_tensor = transform_func(Image.open('/content/people-clothing-segmentation/png_images/IMAGES/img_0002.png')).unsqueeze(0).to(device)\n",
        "    outputs = model(image_tensor)\n",
        "    print(\"Initial predictions:\", outputs)"
      ],
      "metadata": {
        "id": "PsfwXaeeghR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the tiny_dataset and tiny_loader creation\n",
        "!pip install --upgrade torch_xla\n",
        "# Train only on the first few batches for overfitting\n",
        "trainer = pl.Trainer(max_epochs=20, overfit_batches=1)  # Overfit on 3 batches\n",
        "\n",
        "# Use the original train_dataloader with overfit_batches\n",
        "trainer.fit(m_lm, train_dataloaders=m_dm.train_dataloader())"
      ],
      "metadata": {
        "id": "aTge8EA8icHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifaXRb1QJy3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIvxLzYoJzM3"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rajkumarl/people-clothing-segmentation\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "we2LYT3kTjdM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MM2cxhdSSiv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOPNzYcITkIJ"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source and destination paths\n",
        "source_path = path # '/kaggle/input/people-clothing-segmentation'\n",
        "destination_path = '/content/people-clothing-segmentation'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "# Copy the dataset (instead of moving)\n",
        "for item in os.listdir(source_path):\n",
        "    s = os.path.join(source_path, item)\n",
        "    d = os.path.join(destination_path, item)\n",
        "    if os.path.isdir(s):\n",
        "        shutil.copytree(s, d, symlinks=False, ignore=None)\n",
        "    else:\n",
        "        shutil.copy2(s, d)\n",
        "\n",
        "print(f\"Dataset copied from '{source_path}' to '{destination_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHy3nn1DLCJI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def create_image_content_dictionaries(image_dir, mask_dir, class_map):\n",
        "    \"\"\"\n",
        "    Creates dictionaries of image content based on grayscale masks and a class map.\n",
        "\n",
        "    Args:\n",
        "        image_dir (str): Path to the directory containing images.\n",
        "        mask_dir (str): Path to the directory containing masks.\n",
        "        class_map (dict): Dictionary mapping grayscale values to class names.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are image filenames and values are dictionaries\n",
        "              containing detected objects and their bounding boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    image_content_dicts = {}  # Dictionary to store results\n",
        "\n",
        "    # Get lists of image and mask files\n",
        "    image_files = os.listdir(image_dir)\n",
        "    mask_files = os.listdir(mask_dir)\n",
        "\n",
        "    # Regular expression to extract image number from filename\n",
        "    pattern = r\"^img_\\d{4}\\.png$\"  # Or adjust as needed\n",
        "\n",
        "    # Iterate through image files\n",
        "    for image_file in image_files:\n",
        "        if re.match(pattern, image_file):\n",
        "            image_number = image_file.split('_')[1].split('.')[0]  # Get 'xxxx' from 'img_xxxx.png'\n",
        "            mask_file_name = f\"seg_{image_number}.png\"\n",
        "        else:\n",
        "            image_number = image_file.split('.')[0]  # Assuming format like 'xxxx.png'\n",
        "            mask_file_name = f\"{image_number}.png\"\n",
        "\n",
        "        # Check if corresponding mask file exists\n",
        "        mask_path = os.path.join(mask_dir, mask_file_name)\n",
        "        if os.path.exists(mask_path):\n",
        "            # Load image and mask\n",
        "            #image = Image.open(os.path.join(image_dir, image_file)).convert('RGB')\n",
        "            mask = Image.open(mask_path).convert('L')\n",
        "            mask_array = np.array(mask)\n",
        "\n",
        "            # Detect objects and create bounding boxes\n",
        "            objects_in_image = {}  # Dictionary to store objects for this image\n",
        "            unique_values = np.unique(mask_array)\n",
        "\n",
        "            for value in unique_values:\n",
        "                if value in class_map:\n",
        "                    class_name = class_map[value]\n",
        "                    # Get coordinates of pixels with this value\n",
        "                    rows, cols = np.where(mask_array == value)\n",
        "                    # Create bounding box\n",
        "                    if rows.size > 0 and cols.size > 0:  # Check for non-empty mask\n",
        "                        x1, y1 = cols.min(), rows.min()\n",
        "                        x2, y2 = cols.max(), rows.max()\n",
        "                        objects_in_image[class_name] = (x1, y1, x2, y2)\n",
        "                    else:\n",
        "                        print(f\"Warning: Class '{class_name}' has an empty mask in image '{image_file}'.\")  # Debugging\n",
        "\n",
        "            # Store results for this image\n",
        "            image_content_dicts[image_file] = objects_in_image\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: Mask file not found for image '{image_file}'. Skipping this image.\")\n",
        "\n",
        "    return image_content_dicts"
      ],
      "metadata": {
        "id": "AT_ESw_zESvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import pickle  # For saving the dictionary data\n",
        "\n",
        "# ... (Assuming you have the create_image_content_dictionaries function defined) ...\n",
        "\n",
        "# Get the image content dictionaries\n",
        "image_dir = '/content/people-clothing-segmentation/png_images/IMAGES'\n",
        "mask_dir = '/content/people-clothing-segmentation/png_masks/MASKS'\n",
        "image_content_data = create_image_content_dictionaries(image_dir, mask_dir, class_map)\n",
        "\n",
        "# Save the dictionary data to a file\n",
        "output_file = '/content/image_content_data.pkl'\n",
        "with open(output_file, 'wb') as f:\n",
        "    pickle.dump(image_content_data, f)\n",
        "\n",
        "print(f\"Image content data saved to: {output_file}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gesjFEHZQcA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pickle\n",
        "\n",
        "# Load the saved image content data\n",
        "output_file = '/content/image_content_data.pkl'\n",
        "with open(output_file, 'rb') as f:\n",
        "    image_content_data = pickle.load(f)\n",
        "\n",
        "# Iterate through the first 50 images\n",
        "for i, (image_file, content) in enumerate(image_content_data.items()):\n",
        "    if i >= 50:  # Limit to the first 50\n",
        "        break\n",
        "\n",
        "    print(f\"Image: {image_file}\")\n",
        "    if content:  # Check if content is not empty\n",
        "        for class_name, bbox in content.items():\n",
        "            print(f\"  - {class_name}: {bbox}\")\n",
        "    else:\n",
        "        print(\"  - No objects detected\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "16-HXQcaJiDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy import stats\n",
        "\n",
        "def get_mode_color_in_bbox(mask_path, bbox, class_map):\n",
        "    \"\"\"\n",
        "    Retrieves the mode color value within a bounding box in a grayscale mask.\n",
        "\n",
        "    Args:\n",
        "        mask_path (str): Path to the grayscale mask image.\n",
        "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2).\n",
        "        class_map (dict): Dictionary mapping grayscale values to class names.\n",
        "\n",
        "    Returns:\n",
        "        int or None: The mode color value within the bounding box, or None if the\n",
        "                     bounding box is empty or invalid.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load mask image as grayscale\n",
        "    mask = Image.open(mask_path).convert('L')\n",
        "    mask_array = np.array(mask)\n",
        "\n",
        "    # Extract region of interest (ROI) defined by the bounding box\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    roi = mask_array[y1:y2 + 1, x1:x2 + 1]  # Slicing to get ROI\n",
        "\n",
        "    # Check if ROI is empty\n",
        "    if roi.size == 0:\n",
        "        print(\"Warning: Bounding box is empty. Returning None.\")  # Debugging\n",
        "        return None\n",
        "\n",
        "    # Calculate mode color within ROI\n",
        "    # Fix: Check if mode result is scalar and handle it\n",
        "    mode_result = stats.mode(roi, axis=None)\n",
        "    mode_color = mode_result.mode[0] if isinstance(mode_result.mode, np.ndarray) else mode_result.mode\n",
        "\n",
        "    # Convert mode color to class name if desired\n",
        "    if class_map:\n",
        "        class_name = class_map.get(mode_color)  # Get class name if available\n",
        "        # Handle cases where the color isn't in the class map (optional)\n",
        "        if class_name is not None:\n",
        "            print(f\"Mode color in bounding box: {mode_color} (Class: {class_name})\")\n",
        "        else:\n",
        "            print(f\"Mode color in bounding box: {mode_color} (Not found in class map)\")\n",
        "\n",
        "    return mode_color  # Or return class_name if you prefer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S1OniEoRdkrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pickle\n",
        "import os\n",
        "#from mode_color_in_bbox import get_mode_color_in_bbox  # Assuming you saved the function earlier\n",
        "\n",
        "\n",
        "# Load the saved image content data\n",
        "output_file = '/content/image_content_data.pkl'\n",
        "with open(output_file, 'rb') as f:\n",
        "    image_content_data = pickle.load(f)\n",
        "\n",
        "mask_dir = '/content/people-clothing-segmentation/png_masks/MASKS'  # Path to mask directory\n",
        "\n",
        "# Iterate through the first 50 images\n",
        "for i, (image_file, content) in enumerate(image_content_data.items()):\n",
        "    if i >= 50:\n",
        "        break\n",
        "\n",
        "    print(f\"Image: {image_file}\")\n",
        "\n",
        "    # Get mask file path\n",
        "    image_number = image_file.split('_')[1].split('.')[0]  # Extract image number\n",
        "    mask_file_name = f\"seg_{image_number}.png\"  # Construct mask filename\n",
        "    mask_path = os.path.join(mask_dir, mask_file_name)\n",
        "\n",
        "    # Update content dictionary with mode color\n",
        "    for class_name, bbox in content.items():\n",
        "        mode_color = get_mode_color_in_bbox(mask_path, bbox, class_map)  # Get mode color\n",
        "        content[class_name] = (bbox, mode_color)  # Update entry with bbox and mode color\n",
        "\n",
        "    # Print updated content\n",
        "    for class_name, (bbox, mode_color) in content.items():\n",
        "        print(f\"  - {class_name}: {bbox}, Mode Color: {mode_color}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "p1ygQdS6cKUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy import stats\n",
        "\n",
        "def get_mode_color_in_bbox(image_path, bbox):\n",
        "    \"\"\"\n",
        "    Retrieves the mode color value within a bounding box in an image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image.\n",
        "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2).\n",
        "\n",
        "    Returns:\n",
        "        str or None: The mode color value in rgb(R, G, B) format,\n",
        "                      or None if the bounding box is empty or invalid.\n",
        "    \"\"\"\n",
        "    # ... (rest of the function remains the same)\n",
        "\n",
        "    # Ensure mode_color is a valid RGB tuple\n",
        "    if mode_result.mode.size > 0:  # Check if mode_result.mode is not empty\n",
        "        mode_color = tuple(map(int, mode_result.mode[0]))  # Convert to integers and unpack\n",
        "        mode_color_str = f\"rgb({mode_color[0]}, {mode_color[1]}, {mode_color[2]})\"  # Format as rgb(R, G, B)\n",
        "    else:\n",
        "        mode_color_str = None  # Handle cases where mode is empty\n",
        "\n",
        "    return mode_color_str  # Return the formatted string"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CF54DKj8eveQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load image content data\n",
        "with open('/content/image_content_data.pkl', 'rb') as f:\n",
        "    image_content_data = pickle.load(f)\n",
        "\n",
        "# Color ranges (adjust as needed)\n",
        "color_ranges = {\n",
        "    \"blue\": [(0, 0, 100), (100, 100, 255)],  # Example range for blue\n",
        "    # Add more color ranges here...\n",
        "}\n",
        "\n",
        "@app.route('/search', methods=['GET'])\n",
        "def search():\n",
        "    class_name = request.args.get('class_name')\n",
        "    color_filter = request.args.get('color_filter')\n",
        "\n",
        "    results = []\n",
        "    for image_file, content in image_content_data.items():\n",
        "        for detected_class, data in content.items():\n",
        "            if detected_class == class_name:\n",
        "                bbox, mode_color_str, *_ = data  # Get mode_color_str\n",
        "\n",
        "                # Apply color filter if provided\n",
        "                if color_filter:\n",
        "                    lower_bound, upper_bound = color_ranges.get(color_filter)\n",
        "                    if lower_bound and upper_bound:\n",
        "                        # Extract RGB values from mode_color_str\n",
        "                        rgb_values = tuple(map(int, mode_color_str[4:-1].split(',')))  # Extract numbers from rgb(R, G, B)\n",
        "\n",
        "                        # Check if mode_color is within the color range\n",
        "                        is_in_range = all(lower_bound[i] <= rgb_values[i] <= upper_bound[i] for i in range(3))\n",
        "                        if not is_in_range:\n",
        "                            continue  # Skip if not in color range\n",
        "\n",
        "                results.append({\"image_file\": image_file, \"bbox\": bbox, \"mode_color\": mode_color_str})  # Include mode_color\n",
        "                break  # Stop searching for this class in this image\n",
        "\n",
        "    return jsonify(results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ilT-YRJEfeft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install Flask\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load image content data\n",
        "with open('/content/image_content_data.pkl', 'rb') as f:\n",
        "    image_content_data = pickle.load(f)\n",
        "\n",
        "# Color ranges (adjust as needed)\n",
        "color_ranges = {\n",
        "    \"blue\": [(0, 0, 100), (100, 100, 255)],  # Example range for blue\n",
        "    # Add more color ranges here...\n",
        "}\n",
        "\n",
        "@app.route('/search', methods=['GET'])\n",
        "def search():\n",
        "    class_name = request.args.get('class_name')\n",
        "    color_filter = request.args.get('color_filter')\n",
        "\n",
        "    results = []\n",
        "    for image_file, content in image_content_data.items():\n",
        "        for detected_class, data in content.items():\n",
        "            if detected_class == class_name:\n",
        "                bbox, mode_color, *_ = data\n",
        "\n",
        "                # Apply color filter if provided\n",
        "                if color_filter:\n",
        "                    lower_bound, upper_bound = color_ranges.get(color_filter)\n",
        "                    if lower_bound and upper_bound:\n",
        "                        # Check if mode_color is within the color range\n",
        "                        is_in_range = all(lower_bound[i] <= int(mode_color[i]) <= upper_bound[i] for i in range(3))\n",
        "                        if not is_in_range:\n",
        "                            continue  # Skip if not in color range\n",
        "\n",
        "                results.append({\"image_file\": image_file, \"bbox\": bbox})\n",
        "                break  # Stop searching for this class in this image\n",
        "\n",
        "    return jsonify(results)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "g2tz-4bLO0hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbDvhULpcavL"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(accelerator='cuda', max_epochs=1)  # Adjust accelerator as needed\n",
        "trainer.validate(m_lm, datamodule=m_dm)"
      ]
    },
    {
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved image content data\n",
        "output_file = '/content/image_content_data.pkl'\n",
        "with open(output_file, 'rb') as f:\n",
        "    image_content_data = pickle.load(f)\n",
        "\n",
        "# --- Verify data structure and mode_color values ---\n",
        "for i, (image_file, content) in enumerate(image_content_data.items()):\n",
        "    if i >= 5:  # Limit to the first 5 images for verification\n",
        "        break\n",
        "\n",
        "    print(f\"Image: {image_file}\")\n",
        "    for class_name, data in content.items():\n",
        "        print(f\"  - {class_name}: {data}\")  # Print the entire data structure\n",
        "        if isinstance(data, tuple) and len(data) >= 2:\n",
        "            bbox, mode_color, *_ = data\n",
        "            print(f\"    Mode Color: {mode_color}, Type: {type(mode_color)}\")\n",
        "\n",
        "# --- Create the plot ---\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for i, (image_file, content) in enumerate(image_content_data.items()):\n",
        "    if i >= 5:\n",
        "        break\n",
        "\n",
        "    for j, (class_name, data) in enumerate(content.items()):\n",
        "        if j >= 5:\n",
        "            break\n",
        "\n",
        "        if isinstance(data, tuple) and len(data) >= 2:\n",
        "            bbox, mode_color, *_ = data\n",
        "\n",
        "            if isinstance(mode_color, (tuple, list)) and len(mode_color) == 3:\n",
        "                try:\n",
        "                    normalized_color = tuple(int(c) / 255 for c in mode_color)\n",
        "                    rect = patches.Rectangle((i, j), 1, 1, linewidth=1, edgecolor='black', facecolor=normalized_color)\n",
        "                    ax.add_patch(rect)\n",
        "                except (TypeError, ValueError) as e:\n",
        "                    print(f\"Error creating color swatch for {class_name} in {image_file}: {e}\")\n",
        "\n",
        "            text = f\"{class_name}\\n({image_file})\"\n",
        "            ax.text(i + 0.5, j + 0.5, text, ha='center', va='center', color='white', fontsize=8)\n",
        "        else:\n",
        "            print(f\"Warning: Unexpected data format for class '{class_name}' in image '{image_file}'. Skipping.\")\n",
        "\n",
        "ax.set_xlim(-0.5, 5.5)\n",
        "ax.set_ylim(-0.5, 5.5)\n",
        "ax.set_xticks(range(5))\n",
        "ax.set_xticklabels([f\"Image {i+1}\" for i in range(5)])\n",
        "ax.set_yticks(range(5))\n",
        "ax.set_yticklabels([f\"Object {i+1}\" for i in range(5)])\n",
        "ax.set_xlabel(\"Images\")\n",
        "ax.set_ylabel(\"Objects\")\n",
        "ax.set_title(\"Mode Colors of Objects in Images\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_Ilb1mKIjgXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_data_to_csv(data, csv_file_path):\n",
        "  if isinstance(data, pd.DataFrame):\n",
        "      data.to_csv(csv_file_path, index=False)\n",
        "  elif isinstance(data, list):\n",
        "      # Example handling for a list of lists\n",
        "      df = pd.DataFrame(data)\n",
        "      df.to_csv(csv_file_path, index=False)\n",
        "  elif isinstance(data, dict):\n",
        "      # Example handling for a dictionary\n",
        "      df = pd.DataFrame([data])\n",
        "      df.to_csv(csv_file_path, index=False)\n",
        "  else:\n",
        "      raise ValueError(\"Unsupported data type for CSV conversion.\")"
      ],
      "metadata": {
        "id": "x6nwnsUvplLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def load_pickle(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "    return data\n",
        "def convert_pickle_to_csv(pickle_file_path, csv_file_path):\n",
        "    data = load_pickle(pickle_file_path)\n",
        "    process_data_to_csv(data, csv_file_path)"
      ],
      "metadata": {
        "id": "gb3RalcfpxTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_file = \"image_content_data.pkl\"\n",
        "csv_file = \"output.csv\"\n",
        "convert_pickle_to_csv(pickle_file, csv_file)"
      ],
      "metadata": {
        "id": "AKqNlWL6qJsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSQRwl8GZMET"
      },
      "outputs": [],
      "source": [
        "\n",
        "save_model(m_lm.model, '/content/save_model.pth')  # Assuming m_lm.model is the actual model object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkaXVVJr_dV_"
      },
      "outputs": [],
      "source": [
        "model = get_model_global(num_classes=Params.class_num)  # Create an instance of your model\n",
        "checkpoint = torch.load('save_model.pth', map_location=device)  # Load the checkpoint\n",
        "\n",
        "# Load the state dictionary directly without the 'model_state_dict' key\n",
        "model.load_state_dict(checkpoint)  # Load the model's state dictionary\n",
        "\n",
        "model.to(device) # Move to appropriate device\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "image_raw = Image.open('/content/bbb_files_png/png_images/TEST_IMAGES/img_0986.png').convert('RGB')\n",
        "image = transform_func(image_raw)\n",
        "image = image.unsqueeze(0).to(device)\n",
        "with torch.no_grad():  # Disable gradient calculations during inference\n",
        "  predictions = model(image)\n",
        "print(predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYAZ9R8hdH6d"
      },
      "outputs": [],
      "source": [
        "model = get_model_global(num_classes=Params.class_num)  # Create an instance of your model\n",
        "checkpoint = torch.load('save_model.pth', map_location=device)  # Load the checkpoint\n",
        "model.load_state_dict(checkpoint)  # Load the model's state dictionary\n",
        "model.to(device)  # Move to appropriate device\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Get a batch of data from the test dataloader\n",
        "val_dataloader = m_dm.val_dataloader()  # Assuming you have m_dm defined\n",
        "images, targets = next(iter(val_dataloader))  # Get the first batch\n",
        "\n",
        "# Move images to the appropriate device\n",
        "images = images.to(device)\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculations during inference\n",
        "    predictions = model(images)\n",
        "\n",
        "# Loop through predictions for each image in the batch\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(f\"Predictions for image {i}:\")\n",
        "    for key, value in prediction.items():\n",
        "        print(f\"  {key}:\")\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            # If it's a tensor, print its values\n",
        "            print(f\"    {value}\")\n",
        "            print(f\"    Shape: {value.shape}\")\n",
        "            print(f\"    dtype: {value.dtype}\")\n",
        "        else:\n",
        "            # If it's not a tensor, just print its value\n",
        "            print(f\"    {value}\")\n",
        "    print(\"-\" * 20)  # Separator between images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJjXivzwCqdY"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3JCxpEt6Q43"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.utils import draw_segmentation_masks\n",
        "from torchvision.ops import box_iou\n",
        "class Mask_LM(pl.LightningModule):\n",
        "    def __init__(\n",
        "            self,\n",
        "            rand_seed=Params.seed,\n",
        "            img_dim_x=Params.img_dim_x,\n",
        "            img_dim_y=Params.img_dim_y,\n",
        "            num_classes=Params.class_num,\n",
        "            album_bbox_form=Params.album_bbox_form,\n",
        "            class_map=Params.class_map,\n",
        "            batch_size=Params.batch_size,\n",
        "            num_workers=Params.num_workers,\n",
        "            transform_func=transform_func,\n",
        "            learning_rate=Params.learning_rate,\n",
        "            num_epochs=Params.num_epochs\n",
        "\n",
        "    ):\n",
        "        # ... (your existing code) ...\n",
        "        self.freeze_epochs = 2  # Number of epochs to freeze layers\n",
        "        self.unfreeze_epochs = 2  # Number of epochs after which to unfreeze more layers\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        \"\"\"Called at the beginning of each training epoch.\"\"\"\n",
        "        self.current_epoch += 1\n",
        "\n",
        "        # Freeze or unfreeze layers based on the current epoch\n",
        "        if self.current_epoch == 1:\n",
        "            self.freeze_backbone()\n",
        "        elif self.current_epoch % self.unfreeze_epochs == 0:  # Unfreeze more layers every 'unfreeze_epochs'\n",
        "            self.unfreeze_more_layers()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        \"\"\"Freeze all layers except the final classification layers.\"\"\"\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False  # Freeze all parameters\n",
        "\n",
        "        # Unfreeze the final classification layers (you may need to adjust this based on your model's architecture)\n",
        "        for param in self.model.roi_heads.box_predictor.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.model.roi_heads.mask_predictor.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_more_layers(self):\n",
        "        \"\"\"Unfreeze a specific set of layers.\"\"\"\n",
        "        # Adjust this to unfreeze the desired layers progressively\n",
        "        # For example, unfreeze a block of layers in the backbone\n",
        "        for param in self.model.backbone.layer3.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.model.backbone.layer4.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # ... (your other methods) ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPg0xCT8pqjD"
      },
      "outputs": [],
      "source": [
        "from memory_profiler import memory_usage\n",
        "import pytorch_lightning as pl\n",
        "import logging\n",
        "\n",
        "class MemoryMonitor(pl.Callback):\n",
        "    def __init__(self, log_interval=100):\n",
        "        super().__init__()\n",
        "        self.log_interval = log_interval\n",
        "\n",
        "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
        "        if (batch_idx + 1) % self.log_interval == 0:\n",
        "            mem_usage = memory_usage(proc=-1, max_usage=True)[0]  # Get memory usage in MiB\n",
        "            logging.info(f\"Memory usage at batch {batch_idx + 1}: {mem_usage:.2f} MiB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q6NZF7MwBcs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1wB_rssTD4k"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZYRwGHGTsrs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e9KeZsSL8AG"
      },
      "outputs": [],
      "source": [
        "# ... your imports and model loading code ...\n",
        "\n",
        "# Print the model architecture (including the mask head)\n",
        "print(Mask_LM().model)\n",
        "\n",
        "# Access and print the mask head specifically\n",
        "print(Mask_LM().model.roi_heads.mask_predictor)\n",
        "\n",
        "# Access specific layers within the mask head\n",
        "# Example: Print the architecture of the 'conv5_mask' layer\n",
        "print(Mask_LM().model.roi_heads.mask_predictor.conv5_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm2fs0SmPgt5"
      },
      "outputs": [],
      "source": [
        "print(model.roi_heads.mask_predictor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1K4rOdHuUbC"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/version_0/\n",
        "%ls lightning_logs/version_0/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW69E3w9Kmh7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "uniqColors = set()\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/png_masks\"):\n",
        "    for file in files:\n",
        "        if file.endswith(('png')):\n",
        "            try:\n",
        "                path = os.path.join(root, file)\n",
        "                with Image.open(path) as imgge:\n",
        "\n",
        "                    colors = imgge.getdata()\n",
        "                    uniqColors.update(set(colors))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error {e}\")\n",
        "print(uniqColors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYHrBlRuvAXm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx9mwmfw74Jt"
      },
      "outputs": [],
      "source": [
        "print(len(uniqColors))\n",
        "gs_values = np.array(list(uniqColors)).reshape(1, -1)\n",
        "plt.imshow(gs_values, cmap='gray', vmin=0, vmax=255)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qMK-OXGe9pPk"
      },
      "outputs": [],
      "source": [
        "image = Image.open('/content/png_masks/MASKS/seg_0989.png')\n",
        "image = transform_func(image)\n",
        "image = image.cpu().numpy()\n",
        "image = np.transpose(image, (1, 2, 0))\n",
        "image = (image * 0.5) + 0.5\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YJ85Y8T8wA4"
      },
      "source": [
        "#Make csv of classes in images and mode colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXTqf53j4Ijb"
      },
      "outputs": [],
      "source": [
        "def get_mode_color(boxes, img, labels, image_path):\n",
        "  i: int = 0\n",
        "  colors_n_labels = {}\n",
        "  for box in boxes:\n",
        "    xmin, ymin, xmax, ymax = boxes\n",
        "    pixels = img[:, ymin:ymax, xmin:xmax]\n",
        "    mode_color = pixels.mode()\n",
        "    colors_n_labels{\n",
        "        'label': labels[i],\n",
        "        'color': mode_color,\n",
        "        'image path': image_path\n",
        "    }\n",
        "    i += 1\n",
        "  return colors_n_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqIFGw-QAYGf"
      },
      "outputs": [],
      "source": [
        "idx: int = 0\n",
        "for root, dirs, files in os.walk('/content/bbb_files_png/'):\n",
        "  for file in files:\n",
        "    if file.endswith(('png')):\n",
        "      try:\n",
        "        path = os.path.join(root, file)\n",
        "\n",
        "  image_files = os.listdir(Params.image_dir)\n",
        "  image_path = os.path.join(Params.image_dir, image_files[idx])\n",
        "  image_number = Params.image_files[idx].split('_')[1].split('.')[0]\n",
        "  mask_file_name = f\"seg_{image_number}.png\"\n",
        "\n",
        "  mask_path = os.path.join(Params.mask_dir, mask_file_name)\n",
        "  if not os.path.exists(mask_path):\n",
        "    raise FileNotFoundError(f\"Could not find mask file: {mask_path}\")\n",
        "  image = Image.open(image_path).convert('RGB')\n",
        "  mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "  mask = np.array(mask)\n",
        "  labels = []\n",
        "  masks = []\n",
        "  class_num = len(Params.class_map)\n",
        "  for i in range(class_num):\n",
        "    mask_of_class = mask == i #some kinda magic that makes a copy of the mask with 1s where the class number is\n",
        "    if np.any(mask_of_class):\n",
        "      labeled_mask, things_num = ndimage.label(mask_of_class)\n",
        "      for j in range(1, things_num + 1):\n",
        "          object_mask = (labeled_mask == j).astype(np.uint8) # ??? dim correct???\n",
        "          masks.append(torch.tensor(object_mask, dtype=torch.int64))\n",
        "          labels.append(Params.class_map.get(i))\n",
        "  del mask_of_class, labeled_mask\n",
        "\n",
        "  #masks[i, :, :] = mask == i\n",
        "  #self.class_map.get(i, \"background\"\n",
        "  #.expand(Params.batch_size, -1, -1)\n",
        "  boxes = []\n",
        "  for mask in masks:\n",
        "    boxes.append(ops.masks_to_boxes(mask.unsqueeze(0)))\n",
        "  boxes = torch.cat(boxes, dim=0)\n",
        "  transform = transforms.ToTensor()\n",
        "  get_mode_color(boxes, transform(image), labels, image_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}